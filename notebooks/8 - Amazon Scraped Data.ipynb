{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4175ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85fd12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables defined before scraping\n",
    "data_dir = os.path.join(\"..\", \"Datasets\", \"db\")\n",
    "product_t = pd.read_csv(os.path.join(data_dir, \"Products.csv\"))\n",
    "\n",
    "headers = {\"User-Agent\": \"<user-agent>\"\n",
    "          }\n",
    "\n",
    "product_list = product_t[\"product_name\"].unique().tolist()\n",
    "product_details = []\n",
    "product_details_cols = [\"product\", \"product_similarity\", \"price\", \"rating\", \"availability\", \n",
    "                        \"review_count\", \"amazon_product_name\", \"review_date\", \"review_score\", \"review_text\"]\n",
    "amazon_url = \"https://www.amazon.com\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91dc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping Functions - Product Level\n",
    "# function to get product title\n",
    "def get_product_title(soup):\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        # Inner NavigableString Object\n",
    "        title_value = title.string\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_product_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'class':'a-price'}).select_one(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_product_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_product_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_product_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    "    except AttributeError:\n",
    "        available = \"\"\n",
    "    return available\n",
    "\n",
    "# Function to get link that shows all reviews related to the product  \n",
    "def get_product_reviews_link(soup):\n",
    "    try:\n",
    "        review_footer = soup.find_all(\"div\", attrs={\"id\":\"reviews-medley-footer\"})[0]\n",
    "        all_reviews_link = review_footer.find(\"a\").get(\"href\")\n",
    "    except:\n",
    "        all_reviews_link = \"\"\n",
    "    return all_reviews_link\n",
    "\n",
    "\n",
    "\n",
    "## Scraping Functions - Review Level\n",
    "# Function to get date info from product review\n",
    "def get_review_date(soup):\n",
    "    try:\n",
    "        soup_review_date = soup.find(\"span\", attrs={\"class\": \"review-date\"}).text\n",
    "        soup_review_date = soup_review_date[soup_review_date.index(\"on\") + 3:]\n",
    "    except:  \n",
    "        soup_review_date = \"\"\n",
    "    return soup_review_date\n",
    "\n",
    "# Function to get review score from product review\n",
    "def get_review_score(soup):\n",
    "    try:\n",
    "        soup_review_score = soup.select_one('a[title*=\"of 5 stars\"]').get(\"title\")[:3]\n",
    "    except:\n",
    "        soup_review_score = \"\"\n",
    "    return soup_review_score\n",
    "\n",
    "# Function to get review text content from product review\n",
    "def get_review_text(soup):\n",
    "    try:\n",
    "        soup_review_text = soup.find(\"span\", attrs={\"class\": \"review-text-content\"}).\\\n",
    "                                    select_one(\"span > span\").text\n",
    "    except:\n",
    "        soup_review_text = \"\"\n",
    "    return soup_review_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1317fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function\n",
    "# verify similarity of product page and main product scraped for\n",
    "def verify_sim(main_txt, sub_txt):\n",
    "    main_txt, sub_txt = str(main_txt).lower(), str(sub_txt).lower()\n",
    "    print(\"{} VS. {}\".format(main_txt, sub_txt))\n",
    "    sub_txt_elems = sub_txt.split()\n",
    "    main_txt_len = len(main_txt.split())\n",
    "    sim_score = 0\n",
    "    for word_i in sub_txt_elems:\n",
    "        if word_i in main_txt:\n",
    "            sim_score += (1/main_txt_len)\n",
    "    sim_score = 0.6 * sim_score + 0.4 * (1/(main_txt_len))\n",
    "    \n",
    "    return sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f665ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Functions for Running the Web Scraper\n",
    "def get_product_review_data(start_url, url_suffix, product_data):\n",
    "    product_reviews_page_number = 1\n",
    "    global product_details\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        try:\n",
    "            product_reviews_response = requests.get(start_url + url_suffix,\n",
    "                                                    headers=headers)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        pr_soup = BeautifulSoup(product_reviews_response.content, \"html.parser\")\n",
    "        # print(\" - - \" +  start_url + url_suffix, \"- page {}\".format(product_reviews_page_number))\n",
    "        pr_review_section = pr_soup.find(\"div\", attrs={\"id\": \"cm_cr-review_list\"})\n",
    "        if pr_review_section == None:\n",
    "            break\n",
    "        if len(pr_review_section.text.strip()) == 0:\n",
    "            # print(\"- - DONE\")\n",
    "            break\n",
    "        pr_rs_reviews = pr_review_section.find_all(\"div\", attrs={\"class\": \"a-section review aok-relative\"})\n",
    "        pr_rs_nav = pr_review_section.find(\"div\", attrs={\"class\": \"a-form-actions a-spacing-top-extra-large\"})\n",
    "\n",
    "        for pr_rs_review in pr_rs_reviews:\n",
    "            pr_rs_date = get_review_date(pr_rs_review)\n",
    "            pr_rs_score = get_review_score(pr_rs_review)\n",
    "            pr_rs_text = get_review_text(pr_rs_review)\n",
    "            # print(\"- - review date: {}\".format(pr_rs_date))\n",
    "            # print(\"- - review score: {}\".format(pr_rs_score))\n",
    "            # print()\n",
    "            \n",
    "            # ADDING DATA\n",
    "            product_details.append((product_data[\"product_name\"], product_data[\"similarity_score\"], \n",
    "                    product_data[\"product_price\"], product_data[\"product_rating\"], \n",
    "                    product_data[\"product_availability\"], product_data[\"product_review_number\"],\n",
    "                    product_data[\"amazon_product_name\"], pr_rs_date, pr_rs_score, pr_rs_text))\n",
    "\n",
    "        url_suffix = url_suffix.replace(\"cm_cr_getr_d_paging_btm_next_\" + str(product_reviews_page_number), \n",
    "                            \"cm_cr_getr_d_paging_btm_next_\" + str(product_reviews_page_number+1))\n",
    "        url_suffix = url_suffix.replace(\"&pageNumber=\" + str(product_reviews_page_number),\n",
    "                                  \"&pageNumber=\" + str(product_reviews_page_number+1))\n",
    "\n",
    "        product_reviews_page_number += 1\n",
    "        \n",
    "        if product_reviews_page_number == 3:\n",
    "            break\n",
    "        \n",
    "    \n",
    "\n",
    "def get_product_data(main_url, soup, product_name):\n",
    "    # get first 5 links\n",
    "    product_links = list(map(lambda i: i.get('href'), \n",
    "                             soup.find_all(\"a\", \n",
    "                                   attrs={'class':'a-link-normal s-no-outline'})[:random.randint(1, 4)]))\n",
    "    timeout_counter = 0\n",
    "    while len(product_links) == 0: \n",
    "        time.sleep(random.randint(10, 30))\n",
    "        product_links = list(map(lambda i: i.get('href'), \n",
    "                             soup.find_all(\"a\", \n",
    "                                   attrs={'class':'a-link-normal s-no-outline'})[:random.randint(1, 4)]))\n",
    "        \n",
    "        timeout_counter += 1\n",
    "        if timeout_counter == 100:\n",
    "            break\n",
    "            \n",
    "    for pl_i in product_links:\n",
    "        # print(\" - \" + main_url + pl_i)\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        try:\n",
    "            product_page_response = requests.get(main_url + pl_i, headers=headers)\n",
    "        except:\n",
    "            continue\n",
    "        pp_soup = BeautifulSoup(product_page_response.content, \"html.parser\")\n",
    "\n",
    "        pp_title = get_product_title(pp_soup)\n",
    "        # print(\" - Product Title =\", pp_title)\n",
    "\n",
    "        pp_sim_score = str(verify_sim(product_name, pp_title))\n",
    "        # print(\" - Product Similarity Score = {}\".format(pp_sim_score))\n",
    "\n",
    "        pp_price = str(get_product_price(pp_soup))\n",
    "        # print(\" - Product Price =\", pp_price)\n",
    "\n",
    "        pp_rating = str(get_product_rating(pp_soup))\n",
    "        # print(\" - Product Rating =\", pp_rating)\n",
    "\n",
    "        pp_review_num = str(get_product_review_count(pp_soup))\n",
    "        # print(\" - Number of Product Reviews =\", pp_review_num)\n",
    "\n",
    "        pp_availability = get_product_availability(pp_soup)\n",
    "        # print(\" - Availability =\", pp_availability)\n",
    "\n",
    "        pp_reviews_href = get_product_reviews_link(pp_soup)\n",
    "        if len(pp_reviews_href.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            pp_reviews_href = pp_reviews_href.replace(\"ref=cm_cr_dp_d_show_all_btm?\", \"ref=cm_cr_getr_d_paging_btm_next_1?\") + \"&pageNumber=1\"\n",
    "        # print(\" - Product Reviews link: {}\".format(main_url + pp_reviews_href))\n",
    "        # print()\n",
    "        \n",
    "        product_dict = {\"product_name\": product_name,\n",
    "                        \"amazon_product_name\": pp_title, \n",
    "                        \"similarity_score\": pp_sim_score,\n",
    "                        \"product_price\": pp_price,\n",
    "                        \"product_rating\": pp_rating,\n",
    "                        \"product_review_number\": pp_review_num,\n",
    "                        \"product_availability\": pp_availability}\n",
    "        get_product_review_data(main_url, pp_reviews_href, product_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa42b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUN ####\n",
    "def run_amazon_scraper():\n",
    "    for product_i in product_list:\n",
    "        # 1 - product search \n",
    "        print(\"-\" * 100)\n",
    "        print(product_i)\n",
    "        product_parse_url = product_i.replace(\" \", \"+\")\n",
    "        product_search_url = \"https://www.amazon.com/s?k={}&ref=nb_sb_noss_1\".format(product_parse_url)\n",
    "        # print(product_search_url)\n",
    "        response_i = requests.get(product_search_url, headers=headers)\n",
    "        # print(response_i.status_code)\n",
    "        links_soup = BeautifulSoup(response_i.content, \"html.parser\")\n",
    "        get_product_data(amazon_url, links_soup, product_i)\n",
    "\n",
    "run_amazon_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27721074",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(product_details, columns=product_details_cols).to_csv(\"../Datasets/product_amazon_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataEngineeringKer",
   "language": "python",
   "name": "dataengineeringker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
